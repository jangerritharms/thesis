\chapter{Formal Model Definition and implementation details}
\label{chap:model}
The discussion of distributed trust systems requires a well-defined framework. In this chapter we
formally define a model in which we can analyze a new information dissemination mechanism. Also, we
explain in detail how the previously introduced TrustChain architecture is implemented. This should
guide the reader towards a proper understanding of the practical intricacies of our system. 

We shall first reintroduce the
notation defined by Mui in previous work on the subject of reputation systems. Afterwards we
apply that notation to the TrustChain solution which will be the basis of the solution defined
in later chapters of this thesis. Finally, we can specifically point out the shortcomings of that
solutions. The next chapter will then introduce a new extension of TrustChain which will tackle
those shortcomings.


\section{Basic model and notation}
\label{sec:notation}
For this model we use the notation that was defined by Mui in \cite{mui2002computational}. The goal 
is to develop the notation for a model of trust and reputation in a social network. Mui developed 
this notation to study a computation model for trust and reputation which fits the subject of this 
work perfectly. For the sake of simplicity we will further simplify the model and ignore the context
dependence of the social networks, so the definitions assume the reputation and trust are about a
single context.

We first define a social network in general.

\begin{defn}[Social network]
    A social network of size $N$ is a society of a set of $N$ agents that allows for agents to communicate and
    interact with each other. We denote the set of agents as $A = \{a_1, a_2, ... a_N\}$ and 
\end{defn}

This definition of a social network can include any society, so it could be the world wide economy,
or Facebook, but from the previous discussions it should be clear that trust and reputation always 
act in a social network. Without the context of the social network those concepts would be of no
use. In most global scale networks we can not assume full observability, therefore Mui defines, with
reference to the work of Granovetter \cite{granovetter1985economic}, the embedded social network.

\begin{defn}[Embedded social network]
    \label{def:embedded}
    An embedded social network with respect to agent $a_i$ is the unique society of agents $A_i$
    that agent $a_i$ is aware of at certain moment in time.
\end{defn}

It should be clear from definition \ref{def:embedded} that we make no full observability assumption. 
Each agents acts within the subjective embedded social network. 

The social network makes it possible for interactions to happen between agents. We call those
interactions ``encounters''. For the moment we use the simplified definition from Mui and assume
that during an encounter both parties chose an action from the set of cooperation and defection:
 $\alpha \in \{\textit{cooperate}, \textit{defect}\}$. We will later extend this definition to the 
usecase of Trilber. As explained before ({\color{red}WHERE?}) this fits the game of Prisoner's 
Dilemma which is the theoretical basis for reputation systems in general. 

\begin{defn}[Encounter]
    An encounter $e \in E = \alpha^2$ is an interaction between agent $a_i$ and agent $a_j$ such that $a_i$
    executed action $\alpha_i$ and $a_j$ executed action $\alpha_j$.
\end{defn}

An agent's encounters are the evidence on which other agents build their opinion of that particular
agent and therefore are the building block for trust. An agent's behavior in the past encounters of 
an define whether other agents will trust that agent or not. Formally Mui defines that history as
follows.

\begin{defn}[History]
    $D_{j,i} = \{ E^* \}$ is the knowledge that $a_j$ has about previous encounters of agent $a_i$,
    which include at least the direct interactions between the two agents but can also include other
    nteractions of $a_i$ which were ``observed'' by $a_j$.
\end{defn}

That fact that encounters happen within the embedded social network that connects the two parties in
the encounter means that the history does not necessarily include all encounters by a certain agent.
With the definition of the history it is possible to define the two concepts of interest, reputaiton
and trust. Consider the case that an agent $a_i$ is determining the reputation of another agent $a_j$
which is in the embedded social network $A_i$. The reputation of $a_j$ in that embedded social
network $A_i$ is solely depended on the history $D_{i,j}$, the encounters which $a_j$ took part in 
and are known to $a_i$'s embedded social network. Mui then defines reputation $\theta_{i,j}$ simply 
as a value between 0 and 1, where a low value means that $a_i$ thinks $a_j$ has a low intention to
reciprocate and a high value means the opposite.

\begin{defn}[Reputation]
    $\theta_{i,j} | D_{i,j} \in [ 0, 1 ]$ is the reputation of agent $a_j$ as seen by $a_i$ given
    the history $D_{i,j}$. 
\end{defn}

Given this definition we are also able to define the ``true reputation'' $\theta'$ which is the reputation 
as calculated using the complete history of all agents encounters, or $\theta'_j | E \in [ 0,1]$.
Slightly deviating from the model of Mui in order to stay closer to previous work on the specific usecase of 
the TrustChain architecture we will define reputation as a direct function of this history.

\begin{defn}[Reputation function]
    $R: D \times A \rightarrow \theta^N$ is a function that maps from the known history of encounters $D$ to a
    reputation value $\theta$ for each of the $N$ agents in $A$.
\end{defn}

Finally, the definition of trust as given by Mui is the expectation an agent $a_i$ has that another 
agent $a_j$ will reciprocate actions in a future encounter.

Given the above definitions a circular relationship between reputation, trust and reciprocity can be
induced. Acting reciprocatively in an embedded social network increases an agent's reputation, which 
in turn increases the trust other agents have in that agent. More trust should then lead to other 
agent's acting reciprocatively which closes the circle. 

However Mui states explicitly that a ``decrease in any of the three variables should lead to the 
reverse effect'', thus this circular relationship only holds true if the history of actions is to a 
large amount transparent to other agents. Also if agents act purposefully wrong and not according
to the reputation they calculate the effectivity of the system breaks down. In practice this boils 
down to problem of a tamper-proof record of encounters and the dissemination of information about 
those encounters. The next section discusses how this model can be implemented in a system
architecture.

\section{Implementation of the model in TrustChain and Tribler}
The definition of the model given in the previous section is from a theoretical point of view for a
general reputation system. It is not immediately obvious how this model applies to an Implementation
of a distributed reputation system with a specific application. In this section we shall shed light
on how the TrustChain architecture and its application context Tribler fit this model. The results
are summarized in Table~\ref{tab:layersofreputation}. 

\subsection{Application context: Tribler}
In order to fit the model to the application context of Tribler, which is one context in which 
TrustChain can be used we have to map the concepts given in the section~\ref{sec:notation} onto the
concepts in the torrent client context.

An agent in the model of Mui will generally refer to an instance of the Tribler client running on a
machine of a user. A single user can therefore run multiple agents on the same machine. Each
instance of the client has a unique identifier. Instances of a client in a distributed system are 
generally also called nodes. Encounters between agents, in this case the Tribler
clients, are transactions of data on the torrent network or relaying of data for the onion routing. 
In both cases one agents uploads data and another agent downloads data, so the action space is a 
real number, where positive values refer to the amount of data uplaoded and negative numbers to the 
amount of data downloaded. 

The mapping of data upload and download is somewhat difficult to map directly to the cooperate and 
defect actions, and therefore good and bad reputation. In general downloading is seen as consuming 
value and uploading is seen as contributing value to the network. However Tribler's additional layer 
of security adds relaying of data as an action to perform and in that case relaying, uploading and 
downloading the same amount of data, should increase the reputation while downloading more than 
uploading should decrease the reputation. Qualitatively, agents will have a good reputation if they
contribute, that is upload, and relay a lot.

Finally, the difference between trust and reputation is not very straightforward either. However, 
while the reputation is a well defined number, trust can be seen as a value that is more depended
on the network structure. As an example, imagine that we are evaluating our trust in two nodes with 
a similar reputation, that is a similar amount of uploaded and downloaded data. One node has many 
interactions with different nodes, most of which are known to us, while the other node has had 
only a few large interactions with previously unheard of nodes. Taking the definition of Mui as 
basis for trust, our expectation of reciprocity will be higher for the node that had successful 
interactions with nodes that we had successful interactions with than for the other node. This fits
the definition made in \cite{josang2007survey}, who state that ``Trust systems produce a score that
reflects the relying party's subjective view of an entity's trustworthiness, whereas reputation 
systems produce and entity's (public) reputation score as seen by the whole community.'' Therefore
we can define a score, calculated from a certain (subjective) point of view in the network based on 
the known repuatation of nodes, as trust. Such functions have been defined in previous work, for 
example NetFlow in~\cite{trustchain}.

\begin{table}
    \centering
    \caption{Mappings of the theorical model of trust systems to the higher layers of trust systems}
    \label{tab:layersofreputation}
    \begin{tabular}[]{|c|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|} \hline
        Definition & Agent                           & Encounter & Action & Reputation & Trust \\ \hline \hline
        Mui        & individuals in a social network & Event between two agents & \{ \textit{cooperate}, \textit{defect} \} & Value between 0 and 1 showing the perception that suggests an agent's intentions and norms & Expectation that an agent will reciprocate. \\ \hline
        Tribler    & Instance of the Tribler client  & Transaction of data &  Two real numbers showing the upload and download during an encounter & Summed upload and download over history & Subjective value calculated by trust function based on reputation \\ \hline
        TrustChain & & & & & \\ \hline
    \end{tabular}
\end{table}

\subsection{Implementation context: TrustChain}
Similar to the application context the model can be mapped to the implementation layer, that is the 
TrustChain architecture. This way we created a relation between the most basic theoretical layer
of reputation systems, the computational definition by Mui, the implementation layer all the way to
the application layer of Tribler. This allows for discussions of the problem in the context of each 
of these layers without loosing the well-definedness property of concepts.

TrustChain is an implementation of a distributed blockchain-based database specifically designed to create trust 
globally between relative strangers in a digital social network. In TrustChain agents are simply public- 
and private key pairs. Each agent can be identified by the unique public key which is used in each
encounter. Each agent records transactions, in which that agent takes part, as a block on a private 
chain. The transactions are the equivalent of encounters in Mui's notation.

\begin{defn}[Transaction block]
    A transaction block that describes a transaction between agent $a_i$ and $a_j$ can be defined as 
    a 6-tuple $B^{TX}(t) = \lAngle \textit{tx}, \textit{pk}_i, \textit{seq}_i, \textit{pk}_j, \textit{seq}_j, \textit{hash}_{B(t-1)} \rAngle$.
    {\color{red}This is still wrong. We either have to introduce the block proposal/agreement scheme or include two hashes here. }
    where:
        \begin{itemize}
            \item $\textit{tx}$ contains the actions performed during the transaction
            \item $\textit{pk}_i$ is the public key of the initiator of the transactions, agent $a_i$
            \item $\textit{seq}_i$ is the sequence number of the block in the history of interactions of agent $a_i$
            \item $\textit{pk}_j$ is the public key of the responder of the transactions, agent $a_j$
            \item $\textit{seq}_j$ is the sequence number of the block in the history of interactions of agent $a_j$
            \item $\textit{hash}_{B(t-1)}$ is the hash of the previous block
        \end{itemize}
\end{defn}

As TrustChain is designed to be application agnostic, the possible actions in encounters are
not pre-defined but can be anything that can be described by static data. If TrustChain is applied
to the Tribler context a transaction block records the amount of data uploaded and downloaded between
the two parties of the encounter. Trust and reputation are not directly represented in TrustChain as
the system itself is only a way to record encounters, not to interpret them. The interpretation of
records of encounters is left to the application context. Still, just like in the trust function 
defined previously we can assume that in a TrustChain based system the set of encounters, which in 
TrustChain corresponds to the observed transactions is the single input to the function.

The system does allow to define the embedded social network, the society in which an agent acts as
defined by Mui. The embedded social network of an agent $a_i$ in the TrustChain fabric are the 
agents $A_i$ which agent $a_i$ has directly interacted with, that is the public keys that agent $a_i$
is aware of and knows to exist. This also means that in the case of TrustChain the embedded social
network can be described solely by the set of encounters $E_i$ of an agent. 
However in a global network that embedded social network is usually 
a very small fraction of the complete social network. This means that chances are low for an agent to
be aware of the good behavior of other agents which is one of the fundamental properties that a 
reputation system needs to fulfill. This brings the discussion to the problem that was described in
chapter~\ref{chap:problem}.

\section{Strategic manipulations}
\label{sec:manipulations}
The model of Mui can be mapped onto the TrustChain implementation context in a straight-forward 
fashion. We showed that in TrustChain, an agent's embedded social network 
and the agent's true reputation can be inferred simply from the complete set of block that 
the agent had. Blocks are the receipt of an encounter between two agents. 

If the trust system works as expected, a good reputation should have value to agents. All agents on 
the network attempt to obtain a good reputation and be seen as a trusted partner. As such their 
behavior should be meticulously agree with the rules. Yet, if the value is large enough and behaving
well comes at a large enough cost, agents will aim to bend the rules or even break them in a smart 
way in order to get the good reputation for free. As a designer of the trust system it is essential
to predict the possible ways of manipulation and prevent them. In this section we will define several
known types of attacks on trust systems and if applicable define how TrustChain can prevent them.

\subsection{Forking}
Forking is one of the most well-known attacks of blockchain based systems. In the cryptocurrency 
context it is also known as double spending. As defined in Definition \ref{def:fork} an attacking agent 
sends two conflicting transactions to two different partners. As long as the two partners are each not 
aware of the other version of the transaction, both transactions are accepted. This practically 
allows the attacker to reuse some reputation that normally would already have been lost or overwrite
a transaction that would otherwise lower the attacker's reputation.

\begin{defn}[Fork]
    \label{def:fork}
    An agent $a_i$ creates two blocks $B^{TX}$ and $B^{TX'}$, with sequence numbers $\textit{seq}_i$
    and $\textit{seq}_i'$ and partner keys $\textit{pk}_j$ and $\textit{pk}_j'$, respectively. $a_i$
    creates a fork if $\textit{seq}_i = \textit{seq}_i'$ and $\textit{pk}_j \neq \textit{pk}_j'$.
\end{defn}

The fact that agents in the TrustChain fabric are solely responsible for their own chain forking is
easy and cannot be prevented. However in certain circumstances the attack can be detected, putting a
negative mark on the attacker. Namely, if one of the agents who were subject to the attack learn 
about the other version of the block, they can see that the attacker signed two conflicting blocks.
This creates a proof-of-fraud and the agent can ignore the attacker in the future. 

\subsection{Transaction hiding}
Once agents have a longer history they will have records of positive and negative encounters. A 
malicious agent then might try to hide any records of negative encounters, thus boosting the trust 
other have in the attacker. The architecture of TrustChain is made to make such an attack detectable.
The blockchain of the agent creates a tamper-proof, irreversible order for all transactions. Another
agent can therefore require the complete sequence of blocks of a partner before interacting. Any 
missing block will be seen as a malicious request. 

The attack can also be mitigated if the partners of the attacker, who also own the blocks that proof
the negative encounters of the attacker, disseminate that information to their peers. As such agents
may already know about the true reputation of the attacker and decide to ignore that agent in the 
first place.

\subsection{Whitewashing}
Agents that have lost reputation in a significant amount of transaction may decide to create a new 
identity for themselves. Thus they can rid themselves from the bad reputation and start fresh. This 
type of attack is called whitewashing. In the currently deployed version of TrustChain in Tribler 
this attack cannot be prevented as the software is free and new agents do not need registering with
any central institutions.

Still the impact of such attacks can be limited by having some mistrust of new agents joining the 
network. In that case a new agents need to ``pay their dues'' before being accepted by the network 
as equals.

\subsection{Sybil attack}
One of the most serious attacks on trust systems is the Sybil attack. In a Sybil attack the attacker
creates a set of new accounts, called the Sybils. Together with the attacking agent the set of 
agents is called a Sybil region. Those Sybils create transaction records 
between each other without actually performing the transactions with the goal of boosting the 
reputation of one of the agents in the Sybil region. The attack is successful as honest agents 
cannot distinguish between fake and real transaction records. That also makes the attack hard to 
prevent.

Multiple ways have been explored to prevent this attack. One way is to analyze the network topology
and use it to detect Sybil regions. Another way is to increase the cost of registering new identities
in the system. 

\subsection{Other attacks}
Other obvious attacks are for example a direct tampering with a block. Such attacks can be 
detected in a straightforward way by simply recalculating the hash and checking with the following 
block and the signature of the agents.

\section{Dissemination and verification of records}
The possible attacks mentioned in the previous section show that the TrustChain architecture, while
allowing honest agents to detect malicious agents, is not secure in a preventive manner. The security
relies on honest agents to assemble transaction records of other agents and check them against their
current knowledge. We call these two processes dissemination and verification of transaction records.
In this section the problems of these process will be discussed that need to be solved in order to 
fight any malicious behavior in the network.

\subsection{Complete synchronization and scalability}
All of the dissemination mechanisms mentioned in \cite{hedetniemi1988survey} consider the goal of
complete information exchange. This would also be a disireable situation in the context of reputation
systems. If each agent has knowledge of all encounters, they could calculate the true reputation of
all agents and simply agree on whom to trust. The system would also be secure against malicious 
behavior. 

However, a global reputation system that tracks all interactions of all agents creates a huge amount
of data which would need to be transmitted and stored on all agent's devices. This seems like an
unfeasible target. Instead a ``high level'' of information dissemination should be strived for.

It should be clear the whether the state of complete synchronization can be achieved depends on the
rate at which new interactions are recorded and the rate at which information dissemination happens.
In fact, Bitcoin achieves full synchronization (except for the newest blocks which are seen as ``not
yet confirmed'') through restricting the block time and size. With a block size of 4MB and a 10 minute
block time, there is enough time between new blocks such that nodes can synchronize with the updated
state of the system.

\subsection{Second-order manipulative behavior}
Another intricacy of distributed system is that the designer of the system has no control over the 
actual behavior of agents. That is why incentives need to be put in place in order to make it disadvantageous
to deviate from the designed behavior. Given the right incentives and rational agents that try to 
maximize their value function the designer can be sure that no misbehavior will spread as it would 
decrease the value function of those agents.

The problem with the TrustChain system is that while it's security and validity as a trust system 
rely on the dissemination and verification of data, there is no incentive in place to guarantee that agents
engage in any such activity. Actually the opposite is true when assuming that data storage, computation power and 
bandwidth are costly resources in the eyes of agents. It is actually disadvantageous for agents to 
observe interactions of other agents, store them and verify them against their previous knowledge in
order to detect any malicious behavior. It is therefore possible to free-ride, not in the context 
of an application like Tribler, but in terms of information dissemination: agents can decide to not 
share and verify information and still take part in the network as valid agents. 

We can thus define another class of malicious behavior which we will call second-order manipulations 
which is distinct from those mentioned in Section \ref{sec:manipulations}, which we will refer to 
as first-order manipulation. Agent behavior that does not immediately violate any rules for data 
creation but rather does not help the network defend itself against attackers falls into this new 
class. Specifically we define two such manipulations: dissemination free-riders and verification 
free-riders.

\begin{defn}[Dissemination free-rider]
    A dissemination free-riders behave honestly except that they do not actively distribute 
    any transaction records to peers.
\end{defn}

\begin{defn}[Verification free-rider]
    A verification free-riders behave honestly except that they do not verify any data that they 
    receive from the peers.
\end{defn}

Both types of second-order manipulators attack the security of the network through passively not 
helping to defend it. The immediate effects of single agents behaving in this manner are small to 
non-existent; the network security is held up by the honest majority and attackers will still be 
identified. The second-order manipulators ride free on the security provided by their peers. However,
should a large fraction of the network realize that free-riding is possible the security could be
seriously be harmed.

\subsection{Creating incentives}
In order to solve the problem we need to realize how incentives can be created. Bitcoin solves the 
incentive for sharing transaction blocks by given nodes a reward for mining a block. Only if the 
node broadcasts the block, can the award be claimed. Also other nodes want to stay updated on the 
state of the chain in order to mine a block on the most recent chain as new blocks for shorter chains
will not be accepted. 

Instead of giving awards in order to encourage behavior, we can also punish nodes in order to discourage
bad behavior. For example, double spending is discouraged because the records of transactions make 
the attack detectable such that other agent can punish the attacker. 

Another way would be to combine the sharing of information with the reputation system built on top 
of TrustChain. In that way good reputation can not only be built through behaving well in the 
application context but also by being a good agent in the TrustChain network. Helping the network to
defend against malicious nodes by obtaining and spreading information should then be rewarded. 

All ideas require a feature that TrustChain does not offer at the moment: the recording of exchanges
of information. All we have presented before about TrustChain is concerned with transaction in the 
application layer but not the record layer. However, in order to reward the exchange of information 
or punish free-riding on this layer, this information needs to be stored in a tamper-proof manner
just like the transactions. That is the state of an agent, which we argued can be described by the
encounters that the agent had in the past, should include also the information exchange behavior.

% 1. A network in which each agent is acting independently without synchronization. Network state — Agents $P={p1, p2, …}$, encounters E=${e_1, e_2, …}$, encounter $e_1 = <c, p1, p2, s1, s2>$ contribution, related parties


% 2. Each agent has his own state — encounters $E_p = {e_p,1, e_p,2, …}$ which is a subset of E. Also $E^p$ are the transactions in which p is one of the transacting parties.


% 3. In this model agents can perform two actions which change their state: interact and distribute. An interaction creates a new encounter $e_{new}$. A distribute actions transfers knowledge from one agent to the other. While not obvious, such a distribution of knowledge also changes the overall network state, as it creates knowledge of the fact that the receiving agent knows about the received transactions.


% 4. A reputation function takes as input the complete state of an agent, that is all encounters that agent knows about and outputs a reputation score for each known agent. $r_p := E_p -> P \times R$ This reputation function calculates an objective value which is the same for all agents with the same data.



% 5. True reputation: We define the true reputation of agents as the reputation calculated by the reputation function r with as input the complete state of the network.



% 6. The agent takes that reputation to decide whether to give resources to another agent or not. The problem is that the local reputations are not the true reputations and the larger the difference, the more it deteriorates the reputation system. (How can we prove this?!) The goal must be to approach the true reputation.


% 7. There are two ways to approach the true reputation:
%     1. We combine rankings
%         * 
%     2. We obtain more data
%         * 


% 8. From both approaches we find that we actually want to achieve strategy-proof sharing of all data and strategy-proof compilation of data.


% 9. We can enforce behavior by making the alternative disadvantageous. 


% 10. Right now there is not way to check whether an agent helps with verifying transactions or compiling data. We also don’t check if agents present the same rankings to every agent. In order to make not verifying and compiling data disadvantageous we need to make sure that it’s possible to detect such behavior. 


% 11. Possible attacks:
%     1. Together with a group we all list a certain node in our rankings low, such that we 


% 12. Thus we can define the requirements for our system:
%     1. the history of encounters should be immutable
%     2. the history of encounters needs to be ordered for each agent
%     3. the throughput of encounters needs to be horizontally scalable
%     4. fully distributed
%     5. we should be able to detect when an agent does not crawl data, does not report a double spend or does not give the same ranking to other agents